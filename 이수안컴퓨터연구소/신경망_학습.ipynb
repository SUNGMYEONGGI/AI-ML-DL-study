{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VQvNez4qydhL",
        "-7te43hqyiiJ",
        "orUoPmDcymhj",
        "BjmLWgFVysnq",
        "h5Z2LTT_y3i5",
        "wbNDoH_3zbGZ",
        "2P-ib8_RzHTh",
        "PZoyQv_czT7R",
        "HoMXNiXWzts-",
        "DZ79pc4jzw3O",
        "jWmEtX_VnLSI",
        "JEBhczCIz57Q",
        "TzQaaHKKz8sZ",
        "jR-rHaTU0Mga",
        "NiTWfSQ60Zl2",
        "hmmL0VIu0bXq",
        "Cy-ktElI0o5P",
        "VAlq_-6E1nIq",
        "nkTDx8Ah1xHY",
        "i-SK4G262Agn",
        "lplK_x0l2YLh",
        "lmHKd45d2JbJ",
        "IIV_GsoG2eDs",
        "YUDNWwj49byH",
        "WjBRQYlP74GM",
        "MDvtEiD77_gu",
        "e_rNg5Jn8FRA",
        "b7nL8f20x4zl"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9c4cfddf282b446c8b74bde0ff2bb7ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b1af583f36b496d96991172b0a10acc",
              "IPY_MODEL_f268ca95424a4b2c8986383067c4dd1d",
              "IPY_MODEL_ec825168fe9f4e88bdea6a009b7ef34e"
            ],
            "layout": "IPY_MODEL_c624ef73b42e41c5afecdefb98b9e65a"
          }
        },
        "2b1af583f36b496d96991172b0a10acc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_450e3d6e18a9439b929ec1c915235731",
            "placeholder": "​",
            "style": "IPY_MODEL_c42d641ffb31453aac65a81f226049ee",
            "value": "100%"
          }
        },
        "f268ca95424a4b2c8986383067c4dd1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0dce9b1bdaf46d2a81b1343829d11f8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5587104c9365401eab15392f1e2ae030",
            "value": 2
          }
        },
        "ec825168fe9f4e88bdea6a009b7ef34e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b33b79c07cde4b08a7460ef80bc969ec",
            "placeholder": "​",
            "style": "IPY_MODEL_f25ba3d7266c42939a00b37f1ebcb475",
            "value": " 2/2 [03:25&lt;00:00, 102.56s/it]"
          }
        },
        "c624ef73b42e41c5afecdefb98b9e65a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "450e3d6e18a9439b929ec1c915235731": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c42d641ffb31453aac65a81f226049ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0dce9b1bdaf46d2a81b1343829d11f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5587104c9365401eab15392f1e2ae030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b33b79c07cde4b08a7460ef80bc969ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f25ba3d7266c42939a00b37f1ebcb475": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyjyu4FzUAVw"
      },
      "source": [
        "# 신경망 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQvNez4qydhL"
      },
      "source": [
        "## 단순한 신경망 구현 : Logic Gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7te43hqyiiJ"
      },
      "source": [
        "### 필요한 모듈 import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf2F_YbdybBE"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orUoPmDcymhj"
      },
      "source": [
        "### 하이퍼 파라미터(Hyper Parameter)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOAmMxo0ymDF"
      },
      "source": [
        "epochs = 10000\n",
        "lr = 0.1"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjmLWgFVysnq"
      },
      "source": [
        "### 유틸 함수들(Util Functions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4OMFGrjyq1c"
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def mean_squared_error(pred_y, true_y):\n",
        "    return 0.5 * (np.sum((true_y - pred_y)**2))\n",
        "\n",
        "def cross_entropy_error(pred_y, true_y):\n",
        "    if true_y.ndim == 1:\n",
        "        true_y = true_y.reshape(1, -1)\n",
        "        pred_y = pred_y.reshape(1, -1)\n",
        "\n",
        "    delta = 1e-7\n",
        "    return -np.sum(true_y * np.log(pred_y + delta)) / batch_size\n",
        "\n",
        "def cross_entropy_error_for_bin(pred_y, true_y):\n",
        "    return 0.5 * np.sum((-true_y * np.log(pred_y) - (1 - true_y) * np.log(1 - pred_y)))\n",
        "\n",
        "def softmax(a):\n",
        "    exp_a = np.exp(a)\n",
        "    sum_exp_a = np.sum(exp_a)\n",
        "    y = exp_a / sum_exp_a\n",
        "\n",
        "    return y\n",
        "\n",
        "def differential(f, x):\n",
        "    eps = 1e-5\n",
        "    diff_value = np.zeros_like(x)\n",
        "\n",
        "    for i in range(x.shape[0]):\n",
        "        temp_val = x[i]\n",
        "\n",
        "        x[i] = temp_val + eps\n",
        "        f_h1 = f(x)\n",
        "\n",
        "        x[i] = temp_val - eps\n",
        "        f_h2 = f(x)\n",
        "\n",
        "        diff_value[i] = (f_h1 - f_h2) / (2 * eps)\n",
        "        x[i] = temp_val\n",
        "\n",
        "    return diff_value"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5Z2LTT_y3i5"
      },
      "source": [
        "### 신경망"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMTjjYgdy3D8"
      },
      "source": [
        "class LogicGateNet():\n",
        "\n",
        "    def __init__(self):\n",
        "        def weight_init():\n",
        "            np.random.seed(1)\n",
        "            weights = np.random.randn(2)\n",
        "            bias = np.random.rand(1)\n",
        "\n",
        "            return weights, bias\n",
        "\n",
        "        self.weights, self.bias = weight_init()\n",
        "\n",
        "    def predict(self, x):\n",
        "        W = self.weights.reshape(-1, 1)\n",
        "        b = self.bias\n",
        "\n",
        "        pred_y = sigmoid(np.dot(x, W) + b)\n",
        "        return pred_y\n",
        "\n",
        "    def loss(self, x, true_y):\n",
        "        pred_y = self.predict(x)\n",
        "        return cross_entropy_error_for_bin(pred_y, true_y)\n",
        "\n",
        "    def get_gradient(self, x, t):\n",
        "        def loss_grad(grad):\n",
        "            return self.loss(x, t)\n",
        "\n",
        "        grad_W = differential(loss_grad, self.weights)\n",
        "        grad_B = differential(loss_grad, self.bias)\n",
        "\n",
        "        return grad_W, grad_B"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbNDoH_3zbGZ"
      },
      "source": [
        "### AND Gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P-ib8_RzHTh"
      },
      "source": [
        "#### 모델 생성 및 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRiaACA6zGom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8935f947-a1cd-480f-f576-53b0bbb75fab"
      },
      "source": [
        "AND = LogicGateNet()\n",
        "\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "Y = np.array([[0], [0], [0], [1]])\n",
        "\n",
        "train_loss_list = list()\n",
        "\n",
        "for i in range(epochs):\n",
        "    grad_W, grad_B = AND.get_gradient(X, Y)\n",
        "\n",
        "    AND.weights -= lr * grad_W\n",
        "    AND.bias -= lr * grad_B\n",
        "\n",
        "    loss = AND.loss(X, Y)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % 100 == 99:\n",
        "        print(\"Epoch: {}, Cost: {}, Weights: {}, Bias: {}\".format(i+1, loss, AND.weights, AND.bias))"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 100, Cost: 0.6886489498077508, Weights: [1.56426876 0.79168393], Bias: [-2.14871589]\n",
            "Epoch: 200, Cost: 0.4946368603067626, Weights: [2.01360719 1.71241131], Bias: [-3.07894028]\n",
            "Epoch: 300, Cost: 0.3920165980751678, Weights: [2.42841657 2.29753793], Bias: [-3.79103207]\n",
            "Epoch: 400, Cost: 0.3257214374794629, Weights: [2.794852   2.73235738], Bias: [-4.37257095]\n",
            "Epoch: 500, Cost: 0.2786360133470194, Weights: [3.11636193 3.08408364], Bias: [-4.86571237]\n",
            "Epoch: 600, Cost: 0.24328504683857205, Weights: [3.40015395 3.38235762], Bias: [-5.29433736]\n",
            "Epoch: 700, Cost: 0.2157253655246455, Weights: [3.65300561 3.64264217], Bias: [-5.67349792]\n",
            "Epoch: 800, Cost: 0.19363244428376314, Weights: [3.88044124 3.87412053], Bias: [-6.01340133]\n",
            "Epoch: 900, Cost: 0.175532131279099, Weights: [4.08680123 4.08279091], Bias: [-6.32133891]\n",
            "Epoch: 1000, Cost: 0.16043926933305935, Weights: [4.27548114 4.27284863], Bias: [-6.6027234]\n",
            "Epoch: 1100, Cost: 0.1476686479420446, Weights: [4.44915667 4.44737675], Bias: [-6.86169394]\n",
            "Epoch: 1200, Cost: 0.1367282749685519, Weights: [4.60996016 4.60872513], Bias: [-7.10149144]\n",
            "Epoch: 1300, Cost: 0.12725554300944247, Weights: [4.75961186 4.75873513], Bias: [-7.32470312]\n",
            "Epoch: 1400, Cost: 0.11897727167161162, Weights: [4.89951643 4.8988813 ], Bias: [-7.53342813]\n",
            "Epoch: 1500, Cost: 0.11168375002777559, Weights: [5.03083381 5.03036531], Bias: [-7.72939342]\n",
            "Epoch: 1600, Cost: 0.10521134369553156, Weights: [5.15453198 5.15418072], Bias: [-7.91403684]\n",
            "Epoch: 1700, Cost: 0.09943052960167295, Weights: [5.27142641 5.27115916], Bias: [-8.08856824]\n",
            "Epoch: 1800, Cost: 0.09423747514315166, Weights: [5.38221012 5.38200408], Bias: [-8.25401514]\n",
            "Epoch: 1900, Cost: 0.0895479925643004, Weights: [5.48747677 5.48731598], Bias: [-8.41125748]\n",
            "Epoch: 2000, Cost: 0.08529312142996374, Weights: [5.58773858 5.58761171], Bias: [-8.56105453]\n",
            "Epoch: 2100, Cost: 0.08141584958698453, Weights: [5.68344044 5.68333931], Bias: [-8.70406584]\n",
            "Epoch: 2200, Cost: 0.07786864454088172, Weights: [5.77497113 5.77488977], Bias: [-8.84086795]\n",
            "Epoch: 2300, Cost: 0.07461157101604719, Weights: [5.8626723  5.86260627], Bias: [-8.97196762]\n",
            "Epoch: 2400, Cost: 0.07161083868752635, Weights: [5.94684571 5.94679169], Bias: [-9.09781263]\n",
            "Epoch: 2500, Cost: 0.06883766977927742, Weights: [6.02775919 6.02771467], Bias: [-9.21880054]\n",
            "Epoch: 2600, Cost: 0.06626740738858548, Weights: [6.1056515  6.10561456], Bias: [-9.33528587]\n",
            "Epoch: 2700, Cost: 0.06387880699260871, Weights: [6.18073635 6.1807055 ], Bias: [-9.44758609]\n",
            "Epoch: 2800, Cost: 0.06165346877841745, Weights: [6.25320578 6.25317986], Bias: [-9.55598655]\n",
            "Epoch: 2900, Cost: 0.05957537926016232, Weights: [6.32323296 6.32321106], Bias: [-9.66074463]\n",
            "Epoch: 3000, Cost: 0.057630538456882076, Weights: [6.39097457 6.39095596], Bias: [-9.76209332]\n",
            "Epoch: 3100, Cost: 0.055806654605836795, Weights: [6.45657283 6.45655695], Bias: [-9.86024414]\n",
            "Epoch: 3200, Cost: 0.05409289259188596, Weights: [6.52015719 6.52014358], Bias: [-9.95538968]\n",
            "Epoch: 3300, Cost: 0.0524796654082402, Weights: [6.58184585 6.58183412], Bias: [-10.04770581]\n",
            "Epoch: 3400, Cost: 0.05095846032101344, Weights: [6.64174696 6.64173681], Bias: [-10.13735354]\n",
            "Epoch: 3500, Cost: 0.04952169319922555, Weights: [6.69995975 6.69995094], Bias: [-10.22448063]\n",
            "Epoch: 3600, Cost: 0.048162585839960505, Weights: [6.75657552 6.75656783], Bias: [-10.309223]\n",
            "Epoch: 3700, Cost: 0.046875062173298454, Weights: [6.81167836 6.81167164], Bias: [-10.39170595]\n",
            "Epoch: 3800, Cost: 0.045653660050823885, Weights: [6.865346  6.8653401], Bias: [-10.47204525]\n",
            "Epoch: 3900, Cost: 0.04449345596186197, Weights: [6.91765033 6.91764513], Bias: [-10.55034806]\n",
            "Epoch: 4000, Cost: 0.043390000525429094, Weights: [6.96865804 6.96865345], Bias: [-10.62671377]\n",
            "Epoch: 4100, Cost: 0.04233926300520863, Weights: [7.01843108 7.01842701], Bias: [-10.70123471]\n",
            "Epoch: 4200, Cost: 0.041337583412249766, Weights: [7.06702708 7.06702347], Bias: [-10.77399683]\n",
            "Epoch: 4300, Cost: 0.04038163101521239, Weights: [7.11449978 7.11449656], Bias: [-10.84508025]\n",
            "Epoch: 4400, Cost: 0.03946836828215944, Weights: [7.16089933 7.16089645], Bias: [-10.91455976]\n",
            "Epoch: 4500, Cost: 0.038595019444393655, Weights: [7.20627262 7.20627005], Bias: [-10.98250532]\n",
            "Epoch: 4600, Cost: 0.037759043007076776, Weights: [7.25066356 7.25066125], Bias: [-11.04898244]\n",
            "Epoch: 4700, Cost: 0.036958107642173936, Weights: [7.29411333 7.29411125], Bias: [-11.11405254]\n",
            "Epoch: 4800, Cost: 0.036190070988813716, Weights: [7.33666057 7.3366587 ], Bias: [-11.1777733]\n",
            "Epoch: 4900, Cost: 0.03545296096123183, Weights: [7.37834162 7.37833993], Bias: [-11.24019894]\n",
            "Epoch: 5000, Cost: 0.03474495922573187, Weights: [7.41919068 7.41918915], Bias: [-11.30138051]\n",
            "Epoch: 5100, Cost: 0.03406438655934456, Weights: [7.45923996 7.45923858], Bias: [-11.36136612]\n",
            "Epoch: 5200, Cost: 0.03340968984530091, Weights: [7.49851989 7.49851863], Bias: [-11.42020114]\n",
            "Epoch: 5300, Cost: 0.03277943049656419, Weights: [7.53705915 7.537058  ], Bias: [-11.47792845]\n",
            "Epoch: 5400, Cost: 0.032172274127618014, Weights: [7.5748849  7.57488386], Bias: [-11.53458858]\n",
            "Epoch: 5500, Cost: 0.031586981321273126, Weights: [7.61202282 7.61202187], Bias: [-11.59021988]\n",
            "Epoch: 5600, Cost: 0.03102239935722491, Weights: [7.64849725 7.64849638], Bias: [-11.6448587]\n",
            "Epoch: 5700, Cost: 0.03047745478800931, Weights: [7.68433125 7.68433045], Bias: [-11.69853949]\n",
            "Epoch: 5800, Cost: 0.02995114676286101, Weights: [7.71954672 7.71954599], Bias: [-11.75129496]\n",
            "Epoch: 5900, Cost: 0.02944254101322559, Weights: [7.75416447 7.7541638 ], Bias: [-11.80315618]\n",
            "Epoch: 6000, Cost: 0.02895076442496882, Weights: [7.78820425 7.78820364], Bias: [-11.85415269]\n",
            "Epoch: 6100, Cost: 0.028475000131613512, Weights: [7.8216849  7.82168433], Bias: [-11.9043126]\n",
            "Epoch: 6200, Cost: 0.02801448307124006, Weights: [7.85462431 7.85462378], Bias: [-11.95366268]\n",
            "Epoch: 6300, Cost: 0.027568495957024236, Weights: [7.88703956 7.88703907], Bias: [-12.00222843]\n",
            "Epoch: 6400, Cost: 0.027136365617061106, Weights: [7.91894693 7.91894648], Bias: [-12.05003419]\n",
            "Epoch: 6500, Cost: 0.02671745966488514, Weights: [7.95036196 7.95036154], Bias: [-12.09710317]\n",
            "Epoch: 6600, Cost: 0.026311183466131018, Weights: [7.98129949 7.9812991 ], Bias: [-12.14345756]\n",
            "Epoch: 6700, Cost: 0.025916977371472173, Weights: [8.01177369 8.01177334], Bias: [-12.18911855]\n",
            "Epoch: 6800, Cost: 0.02553431418856262, Weights: [8.04179814 8.04179781], Bias: [-12.23410642]\n",
            "Epoch: 6900, Cost: 0.025162696869934047, Weights: [8.07138581 8.07138551], Bias: [-12.27844056]\n",
            "Epoch: 7000, Cost: 0.024801656394794494, Weights: [8.10054913 8.10054885], Bias: [-12.32213955]\n",
            "Epoch: 7100, Cost: 0.02445074982716578, Weights: [8.12930001 8.12929974], Bias: [-12.3652212]\n",
            "Epoch: 7200, Cost: 0.024109558532147288, Weights: [8.15764985 8.1576496 ], Bias: [-12.40770258]\n",
            "Epoch: 7300, Cost: 0.023777686536959722, Weights: [8.18560962 8.18560938], Bias: [-12.44960005]\n",
            "Epoch: 7400, Cost: 0.023454759021929335, Weights: [8.21318981 8.2131896 ], Bias: [-12.49092933]\n",
            "Epoch: 7500, Cost: 0.02314042093077768, Weights: [8.24040053 8.24040033], Bias: [-12.53170551]\n",
            "Epoch: 7600, Cost: 0.022834335688441357, Weights: [8.26725147 8.26725128], Bias: [-12.57194309]\n",
            "Epoch: 7700, Cost: 0.022536184017176542, Weights: [8.29375196 8.29375178], Bias: [-12.61165602]\n",
            "Epoch: 7800, Cost: 0.02224566284259985, Weights: [8.31991095 8.31991079], Bias: [-12.65085771]\n",
            "Epoch: 7900, Cost: 0.021962484281331685, Weights: [8.34573709 8.34573693], Bias: [-12.68956105]\n",
            "Epoch: 8000, Cost: 0.021686374703472944, Weights: [8.37123866 8.37123852], Bias: [-12.72777848]\n",
            "Epoch: 8100, Cost: 0.02141707386357689, Weights: [8.39642369 8.39642355], Bias: [-12.76552195]\n",
            "Epoch: 8200, Cost: 0.021154334094252873, Weights: [8.42129986 8.42129973], Bias: [-12.802803]\n",
            "Epoch: 8300, Cost: 0.02089791955725471, Weights: [8.44587463 8.44587451], Bias: [-12.83963275]\n",
            "Epoch: 8400, Cost: 0.020647605547383444, Weights: [8.47015515 8.47015504], Bias: [-12.87602193]\n",
            "Epoch: 8500, Cost: 0.02040317784478356, Weights: [8.49414835 8.49414824], Bias: [-12.91198087]\n",
            "Epoch: 8600, Cost: 0.02016443211200155, Weights: [8.5178609 8.5178608], Bias: [-12.94751958]\n",
            "Epoch: 8700, Cost: 0.0199311733319202, Weights: [8.54129925 8.54129915], Bias: [-12.98264769]\n",
            "Epoch: 8800, Cost: 0.019703215283653742, Weights: [8.56446962 8.56446953], Bias: [-13.01737452]\n",
            "Epoch: 8900, Cost: 0.01948038005336899, Weights: [8.58737805 8.58737796], Bias: [-13.05170909]\n",
            "Epoch: 9000, Cost: 0.019262497577202605, Weights: [8.61003034 8.61003026], Bias: [-13.08566009]\n",
            "Epoch: 9100, Cost: 0.01904940521406703, Weights: [8.63243214 8.63243206], Bias: [-13.11923596]\n",
            "Epoch: 9200, Cost: 0.01884094734603801, Weights: [8.65458887 8.6545888 ], Bias: [-13.15244484]\n",
            "Epoch: 9300, Cost: 0.01863697500410722, Weights: [8.67650583 8.67650576], Bias: [-13.18529462]\n",
            "Epoch: 9400, Cost: 0.01843734551769339, Weights: [8.6981881  8.69818804], Bias: [-13.21779293]\n",
            "Epoch: 9500, Cost: 0.01824192218596278, Weights: [8.71964064 8.71964058], Bias: [-13.24994718]\n",
            "Epoch: 9600, Cost: 0.018050573969576836, Weights: [8.74086823 8.74086817], Bias: [-13.28176453]\n",
            "Epoch: 9700, Cost: 0.01786317520118178, Weights: [8.76187551 8.76187546], Bias: [-13.31325193]\n",
            "Epoch: 9800, Cost: 0.017679605313567923, Weights: [8.78266699 8.78266694], Bias: [-13.3444161]\n",
            "Epoch: 9900, Cost: 0.017499748584092627, Weights: [8.80324702 8.80324697], Bias: [-13.37526359]\n",
            "Epoch: 10000, Cost: 0.017323493894287664, Weights: [8.82361985 8.8236198 ], Bias: [-13.40580072]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZoyQv_czT7R"
      },
      "source": [
        "#### 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7CvWgc9zREa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1a47e7d-ea57-499b-897a-2f5ab98be95a"
      },
      "source": [
        "print(AND.predict(X))"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.50637836e-06]\n",
            " [1.01289109e-02]\n",
            " [1.01289114e-02]\n",
            " [9.85817171e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoMXNiXWzts-"
      },
      "source": [
        "### OR Gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZ79pc4jzw3O"
      },
      "source": [
        "#### 모델 생성 및 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gnLmAyQzuoL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28c413c7-c318-4de7-e3d2-9040b5229cb6"
      },
      "source": [
        "OR = LogicGateNet()\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "Y_2 = np.array([[0], [1], [1], [1]])\n",
        "\n",
        "train_loss_list = list()\n",
        "\n",
        "for i in range(epochs):\n",
        "    grad_W, grad_B = OR.get_gradient(X, Y_2)\n",
        "\n",
        "    OR.weights -= lr * grad_W\n",
        "    OR.bias -= lr * grad_B\n",
        "\n",
        "    loss = OR.loss(X, Y_2)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % 100 == 99:\n",
        "        print(\"Epoch: {}, Cost: {}, Weights: {}, Bias: {}\".format(i+1, loss, OR.weights, OR.bias))"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 100, Cost: 0.49580923848037245, Weights: [2.45484353 1.40566594], Bias: [-0.14439625]\n",
            "Epoch: 200, Cost: 0.3398674231512476, Weights: [2.98631846 2.39448393], Bias: [-0.67661178]\n",
            "Epoch: 300, Cost: 0.2573360986184237, Weights: [3.45016595 3.08431266], Bias: [-1.03721585]\n",
            "Epoch: 400, Cost: 0.20630142190061632, Weights: [3.85230067 3.60865952], Bias: [-1.30598633]\n",
            "Epoch: 500, Cost: 0.1716549922114765, Weights: [4.20195872 4.03000824], Bias: [-1.52060015]\n",
            "Epoch: 600, Cost: 0.14665018845489367, Weights: [4.50867681 4.38171478], Bias: [-1.6994397]\n",
            "Epoch: 700, Cost: 0.12779768649443757, Weights: [4.78049264 4.68334611], Bias: [-1.8527641]\n",
            "Epoch: 800, Cost: 0.11310517185394649, Weights: [5.0237707 4.9472786], Bias: [-1.98691756]\n",
            "Epoch: 900, Cost: 0.10135180918369109, Weights: [5.24347159 5.18181684], Bias: [-2.10611973]\n",
            "Epoch: 1000, Cost: 0.09174843008614507, Weights: [5.44346811 5.39279833], Bias: [-2.21332947]\n",
            "Epoch: 1100, Cost: 0.08376289253732225, Weights: [5.62680953 5.5844816 ], Bias: [-2.31070587]\n",
            "Epoch: 1200, Cost: 0.07702379120969693, Weights: [5.79592326 5.7600683 ], Bias: [-2.39987434]\n",
            "Epoch: 1300, Cost: 0.07126434272341202, Weights: [5.95276421 5.92202618], Bias: [-2.48208965]\n",
            "Epoch: 1400, Cost: 0.0662881744495673, Weights: [6.09892475 6.07229742], Bias: [-2.5583408]\n",
            "Epoch: 1500, Cost: 0.061947666478230454, Weights: [6.23571596 6.21243794], Bias: [-2.6294211]\n",
            "Epoch: 1600, Cost: 0.05812979389011492, Weights: [6.36422829 6.3437134 ], Bias: [-2.69597653]\n",
            "Epoch: 1700, Cost: 0.05474661199008213, Weights: [6.48537707 6.4671671 ], Bias: [-2.75854002]\n",
            "Epoch: 1800, Cost: 0.051728705095369254, Weights: [6.59993732 6.58366913], Bias: [-2.81755626]\n",
            "Epoch: 1900, Cost: 0.04902057830857761, Weights: [6.70857036 6.69395266], Bias: [-2.87340012]\n",
            "Epoch: 2000, Cost: 0.046577353700281966, Weights: [6.81184468 6.79864117], Bias: [-2.92639049]\n",
            "Epoch: 2100, Cost: 0.04436236088708806, Weights: [6.9102522  6.89826927], Bias: [-2.97680082]\n",
            "Epoch: 2200, Cost: 0.04234535260524903, Weights: [7.00422121 6.99329879], Bias: [-3.02486737]\n",
            "Epoch: 2300, Cost: 0.04050116457040577, Weights: [7.09412675 7.0841314 ], Bias: [-3.07079558]\n",
            "Epoch: 2400, Cost: 0.038808696112830283, Weights: [7.18029898 7.17111856], Bias: [-3.11476523]\n",
            "Epoch: 2500, Cost: 0.037250125723084386, Weights: [7.26302994 7.2545696 ], Bias: [-3.1569345]\n",
            "Epoch: 2600, Cost: 0.03581030087930434, Weights: [7.34257915 7.3347581 ], Bias: [-3.19744328]\n",
            "Epoch: 2700, Cost: 0.0344762587313007, Weights: [7.41917823 7.41192725], Bias: [-3.23641588]\n",
            "Epoch: 2800, Cost: 0.033236846126206476, Weights: [7.49303468 7.48629415], Bias: [-3.27396326]\n",
            "Epoch: 2900, Cost: 0.032082415826793435, Weights: [7.56433513 7.55805339], Bias: [-3.31018484]\n",
            "Epoch: 3000, Cost: 0.0310045817251636, Weights: [7.63324797 7.62738009], Bias: [-3.34517006]\n",
            "Epoch: 3100, Cost: 0.02999602014029317, Weights: [7.69992568 7.69443236], Bias: [-3.37899967]\n",
            "Epoch: 3200, Cost: 0.029050307414244424, Weights: [7.76450671 7.75935346], Bias: [-3.4117468]\n",
            "Epoch: 3300, Cost: 0.028161786319479243, Weights: [7.82711719 7.82227359], Bias: [-3.44347791]\n",
            "Epoch: 3400, Cost: 0.02732545550085983, Weights: [7.88787227 7.88331143], Bias: [-3.47425352]\n",
            "Epoch: 3500, Cost: 0.026536877460650313, Weights: [7.94687741 7.94257543], Bias: [-3.50412897]\n",
            "Epoch: 3600, Cost: 0.0257921015649111, Weights: [8.0042294  8.00016499], Bias: [-3.53315495]\n",
            "Epoch: 3700, Cost: 0.025087599293887014, Weights: [8.06001728 8.05617141], Bias: [-3.56137798]\n",
            "Epoch: 3800, Cost: 0.024420209528489195, Weights: [8.11432315 8.11067877], Bias: [-3.58884092]\n",
            "Epoch: 3900, Cost: 0.023787092108260858, Weights: [8.16722286 8.16376462], Bias: [-3.61558327]\n",
            "Epoch: 4000, Cost: 0.02318568824078412, Weights: [8.21878662 8.21550071], Bias: [-3.64164158]\n",
            "Epoch: 4100, Cost: 0.02261368661478825, Weights: [8.26907955 8.26595348], Bias: [-3.66704967]\n",
            "Epoch: 4200, Cost: 0.022068994282337977, Weights: [8.31816217 8.31518461], Bias: [-3.69183895]\n",
            "Epoch: 4300, Cost: 0.021549711547201488, Weights: [8.3660908  8.36325146], Bias: [-3.71603861]\n",
            "Epoch: 4400, Cost: 0.021054110231991108, Weights: [8.41291793 8.41020747], Bias: [-3.73967583]\n",
            "Epoch: 4500, Cost: 0.02058061480599219, Weights: [8.45869259 8.45610248], Bias: [-3.76277596]\n",
            "Epoch: 4600, Cost: 0.020127785945489193, Weights: [8.50346063 8.50098306], Bias: [-3.78536269]\n",
            "Epoch: 4700, Cost: 0.019694306168679503, Weights: [8.54726498 8.54489281], Bias: [-3.80745818]\n",
            "Epoch: 4800, Cost: 0.019278967246703974, Weights: [8.5901459  8.58787257], Bias: [-3.82908319]\n",
            "Epoch: 4900, Cost: 0.018880659140464294, Weights: [8.63214118 8.62996068], Bias: [-3.85025719]\n",
            "Epoch: 5000, Cost: 0.01849836025198931, Weights: [8.67328637 8.67119315], Bias: [-3.87099849]\n",
            "Epoch: 5100, Cost: 0.018131128812256675, Weights: [8.71361494 8.71160388], Bias: [-3.89132431]\n",
            "Epoch: 5200, Cost: 0.017778095253499392, Weights: [8.7531584  8.75122479], Bias: [-3.91125084]\n",
            "Epoch: 5300, Cost: 0.01743845543833704, Weights: [8.79194651 8.79008597], Bias: [-3.93079337]\n",
            "Epoch: 5400, Cost: 0.017111464634937658, Weights: [8.83000736 8.82821585], Bias: [-3.94996633]\n",
            "Epoch: 5500, Cost: 0.01679643214447736, Weights: [8.86736753 8.8656413 ], Bias: [-3.96878335]\n",
            "Epoch: 5600, Cost: 0.01649271650031154, Weights: [8.90405216 8.90238771], Bias: [-3.98725733]\n",
            "Epoch: 5700, Cost: 0.016199721168795922, Weights: [8.94008507 8.93847916], Bias: [-4.00540046]\n",
            "Epoch: 5800, Cost: 0.015916890692115743, Weights: [8.97548885 8.97393846], Bias: [-4.02322433]\n",
            "Epoch: 5900, Cost: 0.015643707221063508, Weights: [9.01028495 9.00878726], Bias: [-4.0407399]\n",
            "Epoch: 6000, Cost: 0.015379687392320719, Weights: [9.04449373 9.04304611], Bias: [-4.0579576]\n",
            "Epoch: 6100, Cost: 0.01512437951129545, Weights: [9.07813456 9.07673454], Bias: [-4.07488733]\n",
            "Epoch: 6200, Cost: 0.014877361006256999, Weights: [9.11122587 9.10987116], Bias: [-4.09153852]\n",
            "Epoch: 6300, Cost: 0.014638236123579845, Weights: [9.14378521 9.14247365], Bias: [-4.10792013]\n",
            "Epoch: 6400, Cost: 0.01440663383816236, Weights: [9.1758293  9.17455887], Bias: [-4.1240407]\n",
            "Epoch: 6500, Cost: 0.014182205955805516, Weights: [9.2073741 9.2061429], Bias: [-4.13990838]\n",
            "Epoch: 6600, Cost: 0.01396462538748332, Weights: [9.23843483 9.23724108], Bias: [-4.15553094]\n",
            "Epoch: 6700, Cost: 0.013753584577476491, Weights: [9.26902604 9.26786806], Bias: [-4.17091579]\n",
            "Epoch: 6800, Cost: 0.013548794069622904, Weights: [9.29916163 9.29803783], Bias: [-4.18607002]\n",
            "Epoch: 6900, Cost: 0.013349981197963282, Weights: [9.32885488 9.3277638 ], Bias: [-4.20100042]\n",
            "Epoch: 7000, Cost: 0.013156888889141563, Weights: [9.35811853 9.35705875], Bias: [-4.21571345]\n",
            "Epoch: 7100, Cost: 0.01296927456569789, Weights: [9.38696477 9.38593496], Bias: [-4.23021534]\n",
            "Epoch: 7200, Cost: 0.012786909140603242, Weights: [9.41540525 9.41440418], Bias: [-4.24451202]\n",
            "Epoch: 7300, Cost: 0.012609576094028675, Weights: [9.44345119 9.44247766], Bias: [-4.2586092]\n",
            "Epoch: 7400, Cost: 0.012437070624905925, Weights: [9.47111331 9.47016621], Bias: [-4.27251233]\n",
            "Epoch: 7500, Cost: 0.012269198870190937, Weights: [9.49840192 9.4974802 ], Bias: [-4.28622667]\n",
            "Epoch: 7600, Cost: 0.012105777185666458, Weights: [9.52532693 9.52442958], Bias: [-4.29975724]\n",
            "Epoch: 7700, Cost: 0.011946631482527082, Weights: [9.55189785 9.55102391], Bias: [-4.31310889]\n",
            "Epoch: 7800, Cost: 0.011791596615073405, Weights: [9.57812382 9.5772724 ], Bias: [-4.32628627]\n",
            "Epoch: 7900, Cost: 0.011640515814672445, Weights: [9.60401364 9.60318387], Bias: [-4.33929384]\n",
            "Epoch: 8000, Cost: 0.011493240166206915, Weights: [9.62957577 9.62876685], Bias: [-4.3521359]\n",
            "Epoch: 8100, Cost: 0.011349628123324585, Weights: [9.65481837 9.65402952], Bias: [-4.3648166]\n",
            "Epoch: 8200, Cost: 0.011209545058855712, Weights: [9.67974929 9.67897978], Bias: [-4.37733992]\n",
            "Epoch: 8300, Cost: 0.011072862847991976, Weights: [9.70437609 9.70362521], Bias: [-4.38970971]\n",
            "Epoch: 8400, Cost: 0.010939459480947641, Weights: [9.72870607 9.72797316], Bias: [-4.40192967]\n",
            "Epoch: 8500, Cost: 0.010809218703121095, Weights: [9.75274627 9.75203069], Bias: [-4.41400337]\n",
            "Epoch: 8600, Cost: 0.01068202968020776, Weights: [9.77650347 9.77580463], Bias: [-4.42593425]\n",
            "Epoch: 8700, Cost: 0.010557786686531987, Weights: [9.79998423 9.79930154], Bias: [-4.43772565]\n",
            "Epoch: 8800, Cost: 0.010436388814541182, Weights: [9.82319489 9.82252779], Bias: [-4.44938076]\n",
            "Epoch: 8900, Cost: 0.010317739703761335, Weights: [9.84614156 9.84548953], Bias: [-4.4609027]\n",
            "Epoch: 9000, Cost: 0.010201747288022166, Weights: [9.86883014 9.86819268], Bias: [-4.47229446]\n",
            "Epoch: 9100, Cost: 0.010088323559227997, Weights: [9.89126636 9.89064299], Bias: [-4.48355893]\n",
            "Epoch: 9200, Cost: 0.009977384346560486, Weights: [9.91345575 9.912846  ], Bias: [-4.49469893]\n",
            "Epoch: 9300, Cost: 0.009868849109906888, Weights: [9.93540365 9.93480708], Bias: [-4.50571715]\n",
            "Epoch: 9400, Cost: 0.00976264074643964, Weights: [9.95711524 9.95653144], Bias: [-4.51661623]\n",
            "Epoch: 9500, Cost: 0.009658685409513404, Weights: [9.97859553 9.97802409], Bias: [-4.5273987]\n",
            "Epoch: 9600, Cost: 0.009556912338822837, Weights: [9.99984939 9.99928992], Bias: [-4.53806702]\n",
            "Epoch: 9700, Cost: 0.009457253701011992, Weights: [10.02088151 10.02033364], Bias: [-4.54862358]\n",
            "Epoch: 9800, Cost: 0.00935964444016099, Weights: [10.04169646 10.04115983], Bias: [-4.55907068]\n",
            "Epoch: 9900, Cost: 0.00926402213727946, Weights: [10.06229866 10.06177293], Bias: [-4.56941057]\n",
            "Epoch: 10000, Cost: 0.009170326878157744, Weights: [10.0826924  10.08217724], Bias: [-4.57964541]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWmEtX_VnLSI"
      },
      "source": [
        "#### 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwPpOs3-z2vU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78fad0a8-1ed1-4cf1-9971-3c97e1b08f64"
      },
      "source": [
        "print(OR.predict(X))"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.01015436]\n",
            " [0.99594011]\n",
            " [0.99594219]\n",
            " [0.99999983]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEBhczCIz57Q"
      },
      "source": [
        "### NAND Gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzQaaHKKz8sZ"
      },
      "source": [
        "#### 모델 생성 및 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h463QUQRz8PS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9b56f84-0566-4603-9af9-335fbacf2545"
      },
      "source": [
        "NAND = LogicGateNet()\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "Y_3 = np.array([[1], [1], [1], [0]])\n",
        "\n",
        "train_loss_list = list()\n",
        "\n",
        "for i in range(epochs):\n",
        "    grad_W, grad_B = NAND.get_gradient(X, Y_3)\n",
        "\n",
        "    NAND.weights -= lr * grad_W\n",
        "    NAND.bias -= lr * grad_B\n",
        "\n",
        "    loss = NAND.loss(X, Y_3)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % 100 == 99:\n",
        "        print(\"Epoch: {}, Cost: {}, Weights: {}, Bias: {}\".format(i+1, loss, NAND.weights, NAND.bias))"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 100, Cost: 0.7911738653764443, Weights: [-0.48972722 -1.25798774], Bias: [1.74566135]\n",
            "Epoch: 200, Cost: 0.5430490957875922, Weights: [-1.51545093 -1.80261804], Bias: [2.79151756]\n",
            "Epoch: 300, Cost: 0.42125913027366896, Weights: [-2.14614496 -2.26642639], Bias: [3.56506179]\n",
            "Epoch: 400, Cost: 0.3456117101528016, Weights: [-2.607325   -2.66303355], Bias: [4.18521187]\n",
            "Epoch: 500, Cost: 0.293129860518541, Weights: [-2.97696333 -3.00501941], Bias: [4.70528682]\n",
            "Epoch: 600, Cost: 0.2543396786003746, Weights: [-3.28850585 -3.30365261], Bias: [5.1539571]\n",
            "Epoch: 700, Cost: 0.22443918596780807, Weights: [-3.55912171 -3.56778782], Bias: [5.54869527]\n",
            "Epoch: 800, Cost: 0.20067626330797955, Weights: [-3.7989077  -3.80411461], Bias: [5.90108417]\n",
            "Epoch: 900, Cost: 0.1813412551760061, Weights: [-4.01441395 -4.01767547], Bias: [6.21926514]\n",
            "Epoch: 1000, Cost: 0.16530944081725432, Weights: [-4.21019696 -4.21231432], Bias: [6.50920952]\n",
            "Epoch: 1100, Cost: 0.1518081748276198, Weights: [-4.38958932 -4.39100717], Bias: [6.77544101]\n",
            "Epoch: 1200, Cost: 0.14028824033722062, Weights: [-4.55512014 -4.55609563], Bias: [7.02147502]\n",
            "Epoch: 1300, Cost: 0.13034822738486612, Weights: [-4.70876329 -4.70945061], Bias: [7.25010046]\n",
            "Epoch: 1400, Cost: 0.12168786029437936, Weights: [-4.85209363 -4.85258823], Bias: [7.4635683]\n",
            "Epoch: 1500, Cost: 0.11407803183800044, Weights: [-4.98639018 -4.98675285], Bias: [7.66372204]\n",
            "Epoch: 1600, Cost: 0.10734092288853518, Weights: [-5.11270717 -5.11297763], Bias: [7.85209057]\n",
            "Epoch: 1700, Cost: 0.1013364420476447, Weights: [-5.23192444 -5.23212921], Bias: [8.02995591]\n",
            "Epoch: 1800, Cost: 0.09595275266915707, Weights: [-5.34478423 -5.34494142], Bias: [8.19840354]\n",
            "Epoch: 1900, Cost: 0.09109951606075564, Weights: [-5.45191865 -5.45204082], Bias: [8.3583605]\n",
            "Epoch: 2000, Cost: 0.08670298279814456, Weights: [-5.55387048 -5.55396653], Bias: [8.5106247]\n",
            "Epoch: 2100, Cost: 0.08270236794112307, Weights: [-5.6511093 -5.6511856], Bias: [8.65588773]\n",
            "Epoch: 2200, Cost: 0.07904713483394096, Weights: [-5.74404407 -5.74410527], Bias: [8.79475286]\n",
            "Epoch: 2300, Cost: 0.07569493263492169, Weights: [-5.83303315 -5.83308268], Bias: [8.92774938]\n",
            "Epoch: 2400, Cost: 0.07261001129471885, Weights: [-5.91839226 -5.91843267], Bias: [9.05534418]\n",
            "Epoch: 2500, Cost: 0.06976199000771223, Weights: [-6.00040101 -6.00043423], Bias: [9.17795111]\n",
            "Epoch: 2600, Cost: 0.06712489062109024, Weights: [-6.07930816 -6.07933566], Bias: [9.29593867]\n",
            "Epoch: 2700, Cost: 0.06467637192338181, Weights: [-6.15533599 -6.1553589 ], Bias: [9.40963641]\n",
            "Epoch: 2800, Cost: 0.0623971178378568, Weights: [-6.2286839  -6.22870312], Bias: [9.51934014]\n",
            "Epoch: 2900, Cost: 0.060270344678841244, Weights: [-6.29953148 -6.29954768], Bias: [9.6253164]\n",
            "Epoch: 3000, Cost: 0.05828140135043117, Weights: [-6.36804097 -6.36805471], Bias: [9.72780614]\n",
            "Epoch: 3100, Cost: 0.05641744270597607, Weights: [-6.43435946 -6.43437117], Bias: [9.82702793]\n",
            "Epoch: 3200, Cost: 0.05466716094986597, Weights: [-6.49862071 -6.49863073], Bias: [9.92318057]\n",
            "Epoch: 3300, Cost: 0.0530205634254324, Weights: [-6.56094668 -6.5609553 ], Bias: [10.01644542]\n",
            "Epoch: 3400, Cost: 0.051468787727726645, Weights: [-6.62144889 -6.62145634], Bias: [10.10698837]\n",
            "Epoch: 3500, Cost: 0.0500039470453012, Weights: [-6.68022957 -6.68023603], Bias: [10.19496156]\n",
            "Epoch: 3600, Cost: 0.048619000132545415, Weights: [-6.73738266 -6.73738829], Bias: [10.2805048]\n",
            "Epoch: 3700, Cost: 0.04730764146613743, Weights: [-6.79299468 -6.79299959], Bias: [10.36374689]\n",
            "Epoch: 3800, Cost: 0.04606420803259155, Weights: [-6.84714546 -6.84714977], Bias: [10.44480677]\n",
            "Epoch: 3900, Cost: 0.044883599888873534, Weights: [-6.89990886 -6.89991265], Bias: [10.52379442]\n",
            "Epoch: 4000, Cost: 0.04376121218465362, Weights: [-6.95135331 -6.95135666], Bias: [10.60081181]\n",
            "Epoch: 4100, Cost: 0.04269287676782316, Weights: [-7.00154233 -7.0015453 ], Bias: [10.67595361]\n",
            "Epoch: 4200, Cost: 0.041674811837030125, Weights: [-7.05053501 -7.05053763], Bias: [10.74930788]\n",
            "Epoch: 4300, Cost: 0.040703578379912245, Weights: [-7.09838637 -7.0983887 ], Bias: [10.82095667]\n",
            "Epoch: 4400, Cost: 0.039776042356434935, Weights: [-7.14514776 -7.14514984], Bias: [10.89097655]\n",
            "Epoch: 4500, Cost: 0.03888934176393788, Weights: [-7.19086717 -7.19086904], Bias: [10.95943908]\n",
            "Epoch: 4600, Cost: 0.03804085786716033, Weights: [-7.23558951 -7.23559118], Bias: [11.02641123]\n",
            "Epoch: 4700, Cost: 0.037228189991894506, Weights: [-7.27935685 -7.27935836], Bias: [11.09195581]\n",
            "Epoch: 4800, Cost: 0.036449133379782145, Weights: [-7.3222087  -7.32221006], Bias: [11.15613172]\n",
            "Epoch: 4900, Cost: 0.0357016596794471, Weights: [-7.36418217 -7.36418339], Bias: [11.21899434]\n",
            "Epoch: 5000, Cost: 0.034983899716647206, Weights: [-7.40531215 -7.40531326], Bias: [11.28059578]\n",
            "Epoch: 5100, Cost: 0.034294128238959505, Weights: [-7.44563155 -7.44563255], Bias: [11.34098512]\n",
            "Epoch: 5200, Cost: 0.03363075037676838, Weights: [-7.48517136 -7.48517227], Bias: [11.40020865]\n",
            "Epoch: 5300, Cost: 0.03299228959977718, Weights: [-7.52396087 -7.52396169], Bias: [11.45831008]\n",
            "Epoch: 5400, Cost: 0.03237737698066072, Weights: [-7.56202774 -7.56202849], Bias: [11.51533072]\n",
            "Epoch: 5500, Cost: 0.031784741603430054, Weights: [-7.59939815 -7.59939883], Bias: [11.57130965]\n",
            "Epoch: 5600, Cost: 0.031213201976626245, Weights: [-7.63609688 -7.6360975 ], Bias: [11.62628389]\n",
            "Epoch: 5700, Cost: 0.030661658331471574, Weights: [-7.67214742 -7.672148  ], Bias: [11.68028852]\n",
            "Epoch: 5800, Cost: 0.030129085700337968, Weights: [-7.70757207 -7.7075726 ], Bias: [11.73335684]\n",
            "Epoch: 5900, Cost: 0.029614527685008656, Weights: [-7.74239199 -7.74239248], Bias: [11.78552046]\n",
            "Epoch: 6000, Cost: 0.02911709083596649, Weights: [-7.7766273  -7.77662775], Bias: [11.83680943]\n",
            "Epoch: 6100, Cost: 0.02863593957445233, Weights: [-7.81029713 -7.81029754], Bias: [11.88725235]\n",
            "Epoch: 6200, Cost: 0.028170291596583734, Weights: [-7.8434197  -7.84342008], Bias: [11.93687643]\n",
            "Epoch: 6300, Cost: 0.02771941370794454, Weights: [-7.87601236 -7.8760127 ], Bias: [11.98570759]\n",
            "Epoch: 6400, Cost: 0.02728261804159808, Weights: [-7.90809165 -7.90809197], Bias: [12.03377057]\n",
            "Epoch: 6500, Cost: 0.026859258619602516, Weights: [-7.93967337 -7.93967366], Bias: [12.08108895]\n",
            "Epoch: 6600, Cost: 0.0264487282219833, Weights: [-7.97077258 -7.97077286], Bias: [12.12768527]\n",
            "Epoch: 6700, Cost: 0.02605045553188083, Weights: [-8.00140369 -8.00140395], Bias: [12.17358105]\n",
            "Epoch: 6800, Cost: 0.025663902528508215, Weights: [-8.03158048 -8.03158071], Bias: [12.21879687]\n",
            "Epoch: 6900, Cost: 0.025288562103741412, Weights: [-8.0613161  -8.06131632], Bias: [12.26335243]\n",
            "Epoch: 7000, Cost: 0.024923955879765654, Weights: [-8.09062318 -8.09062339], Bias: [12.30726658]\n",
            "Epoch: 7100, Cost: 0.0245696322089552, Weights: [-8.11951381 -8.119514  ], Bias: [12.35055738]\n",
            "Epoch: 7200, Cost: 0.024225164337790792, Weights: [-8.14799955 -8.14799973], Bias: [12.39324215]\n",
            "Epoch: 7300, Cost: 0.02389014871997055, Weights: [-8.17609152 -8.17609168], Bias: [12.43533749]\n",
            "Epoch: 7400, Cost: 0.023564203464500378, Weights: [-8.20380037 -8.20380052], Bias: [12.47685934]\n",
            "Epoch: 7500, Cost: 0.023246966906257056, Weights: [-8.23113634 -8.23113648], Bias: [12.517823]\n",
            "Epoch: 7600, Cost: 0.02293809628829442, Weights: [-8.25810925 -8.25810939], Bias: [12.55824317]\n",
            "Epoch: 7700, Cost: 0.022637266545295732, Weights: [-8.28472856 -8.28472869], Bias: [12.59813397]\n",
            "Epoch: 7800, Cost: 0.022344169179838246, Weights: [-8.31100335 -8.31100347], Bias: [12.63750901]\n",
            "Epoch: 7900, Cost: 0.022058511222924775, Weights: [-8.33694237 -8.33694248], Bias: [12.67638134]\n",
            "Epoch: 8000, Cost: 0.02178001427165715, Weights: [-8.36255403 -8.36255413], Bias: [12.71476356]\n",
            "Epoch: 8100, Cost: 0.021508413597405372, Weights: [-8.38784642 -8.38784652], Bias: [12.75266779]\n",
            "Epoch: 8200, Cost: 0.021243457318600625, Weights: [-8.41282737 -8.41282747], Bias: [12.7901057]\n",
            "Epoch: 8300, Cost: 0.020984905632833284, Weights: [-8.4375044  -8.43750448], Bias: [12.82708855]\n",
            "Epoch: 8400, Cost: 0.02073253010304351, Weights: [-8.46188475 -8.46188484], Bias: [12.8636272]\n",
            "Epoch: 8500, Cost: 0.020486112993836125, Weights: [-8.48597545 -8.48597553], Bias: [12.89973213]\n",
            "Epoch: 8600, Cost: 0.020245446653604247, Weights: [-8.50978324 -8.50978331], Bias: [12.93541344]\n",
            "Epoch: 8700, Cost: 0.020010332938849858, Weights: [-8.53331464 -8.53331471], Bias: [12.97068089]\n",
            "Epoch: 8800, Cost: 0.019780582677436702, Weights: [-8.55657597 -8.55657604], Bias: [13.00554391]\n",
            "Epoch: 8900, Cost: 0.01955601516787917, Weights: [-8.57957331 -8.57957338], Bias: [13.04001162]\n",
            "Epoch: 9000, Cost: 0.019336457711436092, Weights: [-8.60231256 -8.60231261], Bias: [13.07409283]\n",
            "Epoch: 9100, Cost: 0.019121745175034562, Weights: [-8.62479939 -8.62479945], Bias: [13.10779605]\n",
            "Epoch: 9200, Cost: 0.018911719582459886, Weights: [-8.64703933 -8.64703938], Bias: [13.14112952]\n",
            "Epoch: 9300, Cost: 0.018706229731596344, Weights: [-8.6690377  -8.66903775], Bias: [13.17410123]\n",
            "Epoch: 9400, Cost: 0.01850513083613339, Weights: [-8.69079967 -8.69079971], Bias: [13.20671889]\n",
            "Epoch: 9500, Cost: 0.01830828418950299, Weights: [-8.71233022 -8.71233026], Bias: [13.23898997]\n",
            "Epoch: 9600, Cost: 0.018115556849826256, Weights: [-8.73363421 -8.73363425], Bias: [13.27092174]\n",
            "Epoch: 9700, Cost: 0.017926821344218326, Weights: [-8.75471631 -8.75471635], Bias: [13.30252119]\n",
            "Epoch: 9800, Cost: 0.017741955390967516, Weights: [-8.77558109 -8.77558113], Bias: [13.33379515]\n",
            "Epoch: 9900, Cost: 0.017560841638570093, Weights: [-8.79623295 -8.79623299], Bias: [13.36475021]\n",
            "Epoch: 10000, Cost: 0.017383367420255286, Weights: [-8.81667617 -8.8166762 ], Bias: [13.39539277]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR-rHaTU0Mga"
      },
      "source": [
        "#### 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpzKW6sm0Ghp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd8a1a3-f3d3-4a57-b63d-08e16cb6f1f1"
      },
      "source": [
        "print(NAND.predict(X))"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.99999848]\n",
            " [0.9898363 ]\n",
            " [0.9898363 ]\n",
            " [0.01423156]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiTWfSQ60Zl2"
      },
      "source": [
        "### XOR Gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmmL0VIu0bXq"
      },
      "source": [
        "#### 모델 생성 및 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CGm0r1M0a9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a98a93c-64ee-4a66-c0a2-8b3acc80fce5"
      },
      "source": [
        "XOR = LogicGateNet()\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "Y_4 = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "train_loss_list = list()\n",
        "\n",
        "for i in range(epochs):\n",
        "    grad_W, grad_B = XOR.get_gradient(X, Y_4)\n",
        "\n",
        "    XOR.weights -= lr * grad_W\n",
        "    XOR.bias -= lr * grad_B\n",
        "\n",
        "    loss = XOR.loss(X, Y_4)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % 100 == 99:\n",
        "        print(\"Epoch: {}, Cost: {}, Weights: {}, Bias: {}\".format(i+1, loss, XOR.weights, XOR.bias))"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 100, Cost: 1.402685224545991, Weights: [ 0.47012771 -0.19931523], Bias: [-0.16097708]\n",
            "Epoch: 200, Cost: 1.3879445622846385, Weights: [ 0.1572739  -0.03387161], Bias: [-0.07321056]\n",
            "Epoch: 300, Cost: 1.386492030048373, Weights: [0.05525161 0.00089673], Bias: [-0.03330094]\n",
            "Epoch: 400, Cost: 1.3863236205351996, Weights: [0.02049628 0.00504503], Bias: [-0.01514784]\n",
            "Epoch: 500, Cost: 1.386299474364679, Weights: [0.0080051  0.00361297], Bias: [-0.00689034]\n",
            "Epoch: 600, Cost: 1.3862953430687444, Weights: [0.00326661 0.00201812], Bias: [-0.00313421]\n",
            "Epoch: 700, Cost: 1.3862945581495083, Weights: [0.00137938 0.00102449], Bias: [-0.00142566]\n",
            "Epoch: 800, Cost: 1.38629440139037, Weights: [0.00059716 0.00049628], Bias: [-0.00064849]\n",
            "Epoch: 900, Cost: 1.3862943694120307, Weights: [0.00026303 0.00023435], Bias: [-0.00029498]\n",
            "Epoch: 1000, Cost: 1.386294362832352, Weights: [0.0001172  0.00010905], Bias: [-0.00013418]\n",
            "Epoch: 1100, Cost: 1.3862943614739491, Weights: [5.26138665e-05 5.02968121e-05], Bias: [-6.10332729e-05]\n",
            "Epoch: 1200, Cost: 1.3862943611931267, Weights: [2.37348241e-05 2.30761874e-05], Bias: [-2.77622281e-05]\n",
            "Epoch: 1300, Cost: 1.3862943611350418, Weights: [1.07400721e-05 1.05528495e-05], Bias: [-1.26282019e-05]\n",
            "Epoch: 1400, Cost: 1.3862943611230256, Weights: [4.86937263e-06 4.81615725e-06], Bias: [-5.74418962e-06]\n",
            "Epoch: 1500, Cost: 1.3862943611205392, Weights: [2.21038849e-06 2.19527263e-06], Bias: [-2.61285776e-06]\n",
            "Epoch: 1600, Cost: 1.3862943611200247, Weights: [1.00414449e-06 9.99846652e-07], Bias: [-1.18851378e-06]\n",
            "Epoch: 1700, Cost: 1.3862943611199183, Weights: [4.56382658e-07 4.55167905e-07], Bias: [-5.4061649e-07]\n",
            "Epoch: 1800, Cost: 1.3862943611198966, Weights: [2.07495081e-07 2.07145192e-07], Bias: [-2.4591667e-07]\n",
            "Epoch: 1900, Cost: 1.3862943611198917, Weights: [9.43555829e-08 9.42588251e-08], Bias: [-1.11860571e-07]\n",
            "Epoch: 2000, Cost: 1.3862943611198908, Weights: [4.29100684e-08 4.28865853e-08], Bias: [-5.0880461e-08]\n",
            "Epoch: 2100, Cost: 1.3862943611198908, Weights: [1.95176693e-08 1.95130599e-08], Bias: [-2.31470898e-08]\n",
            "Epoch: 2200, Cost: 1.3862943611198906, Weights: [8.87396112e-09 8.88156426e-09], Bias: [-1.05249642e-08]\n",
            "Epoch: 2300, Cost: 1.3862943611198906, Weights: [4.02783762e-09 4.04099187e-09], Bias: [-4.78733164e-09]\n",
            "Epoch: 2400, Cost: 1.3862943611198906, Weights: [1.83514715e-09 1.84497073e-09], Bias: [-2.17941776e-09]\n",
            "Epoch: 2500, Cost: 1.3862943611198906, Weights: [8.02639732e-10 8.44659787e-10], Bias: [-9.84817783e-10]\n",
            "Epoch: 2600, Cost: 1.3862943611198906, Weights: [3.79644760e-10 3.67263886e-10], Bias: [-4.47469839e-10]\n",
            "Epoch: 2700, Cost: 1.3862943611198906, Weights: [1.92017069e-10 1.60762403e-10], Bias: [-2.18763896e-10]\n",
            "Epoch: 2800, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 2900, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 3000, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 3100, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 3200, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 3300, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 3400, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 3500, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 3600, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 3700, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 3800, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 3900, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 4000, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 4100, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 4200, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 4300, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 4400, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 4500, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 4600, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 4700, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 4800, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 4900, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 5000, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 5100, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 5200, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 5300, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 5400, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 5500, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 5600, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 5700, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 5800, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 5900, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 6000, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 6100, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 6200, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 6300, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 6400, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 6500, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 6600, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 6700, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 6800, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 6900, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 7000, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 7100, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 7200, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 7300, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 7400, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 7500, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 7600, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 7700, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 7800, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 7900, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 8000, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 8100, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 8200, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 8300, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 8400, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 8500, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 8600, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 8700, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 8800, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 8900, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 9000, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 9100, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 9200, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 9300, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 9400, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 9500, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 9600, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 9700, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 9800, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 9900, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n",
            "Epoch: 10000, Cost: 1.3862943611198906, Weights: [1.23183241e-10 1.42998835e-10], Bias: [-1.38827838e-10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy-ktElI0o5P"
      },
      "source": [
        "#### 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWAJAJ_T0oqm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c516ee4-40cc-498d-a709-648e413ccbc7"
      },
      "source": [
        "print(XOR.predict(X))"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAlq_-6E1nIq"
      },
      "source": [
        "#### 2층 신경망으로 XOR 게이트 구현(1)\n",
        "\n",
        "- 얕은 신경망, Shallow Neural Network\n",
        "\n",
        "- 두 논리게이트(NAND, OR)를 통과하고  \n",
        "  AND 게이트로 합쳐서 구현\n",
        "\n",
        "- 06 신경망 구조 참고"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mr7nYMG20jTo"
      },
      "source": [
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "Y_5 = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "s1 = NAND.predict(X)\n",
        "s2 = OR.predict(X)\n",
        "X_2 = np.array([s1, s2]).T.reshape(-1, 2)"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkTDx8Ah1xHY"
      },
      "source": [
        "#### 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK2iD5A91yWQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ddd9515-5a35-441a-a8b5-87a578ebeb25"
      },
      "source": [
        "print(AND.predict(X_2))"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.01106769]\n",
            " [0.98395109]\n",
            " [0.98395138]\n",
            " [0.01146859]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-SK4G262Agn"
      },
      "source": [
        "#### 2층 신경망으로 XOR 게이트 구현(2)\n",
        "- 클래스로 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RpnHCRZ1zwr"
      },
      "source": [
        "class XORNet():\n",
        "    def __init__(self):\n",
        "        np.random.seed(1)\n",
        "\n",
        "        def weight_init():\n",
        "            params = {}\n",
        "            params['w_1'] = np.random.randn(2)\n",
        "            params['b_1'] = np.random.rand(2)\n",
        "            params['w_2'] = np.random.randn(2)\n",
        "            params['b_2'] = np.random.rand(1)\n",
        "            return params\n",
        "\n",
        "        self.params = weight_init()\n",
        "\n",
        "    def predict(self, x):\n",
        "        W_1, W_2 = self.params['w_1'].reshape(-1, 1), self.params['w_2'].reshape(-1, 1)\n",
        "        B_1, B_2 = self.params['b_1'], self.params['b_2']\n",
        "\n",
        "        A1 = np.dot(x, W_1) + B_1\n",
        "        Z1 = sigmoid(A1)\n",
        "        A2 = np.dot(Z1, W_2) + B_2\n",
        "        pred_y = sigmoid(A2)\n",
        "\n",
        "        return pred_y\n",
        "\n",
        "    def loss(self, x, true_y):\n",
        "        pred_y = self.predict(x)\n",
        "        return cross_entropy_error_for_bin(pred_y, true_y)\n",
        "\n",
        "    def get_gradient(self, x, t):\n",
        "        def loss_grad(grad):\n",
        "            return self.loss(x ,t)\n",
        "\n",
        "        grads = {}\n",
        "        grads['w_1'] = differential(loss_grad, self.params['w_1'])\n",
        "        grads['b_1'] = differential(loss_grad, self.params['b_1'])\n",
        "        grads['w_2'] = differential(loss_grad, self.params['w_2'])\n",
        "        grads['b_2'] = differential(loss_grad, self.params['b_2'])\n",
        "\n",
        "        return grads"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lplK_x0l2YLh"
      },
      "source": [
        "#### 하이퍼 파라미터(Hyper Parameter)\n",
        "- 재조정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf-3wWSv2b7l"
      },
      "source": [
        "lr = 0.3"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmHKd45d2JbJ"
      },
      "source": [
        "#### 모델 생성 및 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQNd3XVd2Gj7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16c232e0-facf-4b3f-b3cd-2f416cab488d"
      },
      "source": [
        "XOR = XORNet()\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "Y_5 = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "train_loss_list = list()\n",
        "\n",
        "for i in range(epochs):\n",
        "    grads = XOR.get_gradient(X, Y_5)\n",
        "\n",
        "    for key in ('w_1', 'b_1', 'w_2', 'b_2'):\n",
        "        XOR.params[key] -= lr * grads[key]\n",
        "\n",
        "    loss = XOR.loss(X, Y_5)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % 100 == 99:\n",
        "        print(\"Epoch: {}, Cost: {}\".format(i+1, loss))"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 100, Cost: 1.3535614442452921\n",
            "Epoch: 200, Cost: 1.2827154568264447\n",
            "Epoch: 300, Cost: 0.8968907893084717\n",
            "Epoch: 400, Cost: 0.33871971414018315\n",
            "Epoch: 500, Cost: 0.18121344476789647\n",
            "Epoch: 600, Cost: 0.11991186457736537\n",
            "Epoch: 700, Cost: 0.08861936864958647\n",
            "Epoch: 800, Cost: 0.06992180653218366\n",
            "Epoch: 900, Cost: 0.05758041353140311\n",
            "Epoch: 1000, Cost: 0.04886093568479621\n",
            "Epoch: 1100, Cost: 0.04238942291993551\n",
            "Epoch: 1200, Cost: 0.03740417747756614\n",
            "Epoch: 1300, Cost: 0.033450534019909474\n",
            "Epoch: 1400, Cost: 0.030241017021010966\n",
            "Epoch: 1500, Cost: 0.027585288870314584\n",
            "Epoch: 1600, Cost: 0.025352482678047083\n",
            "Epoch: 1700, Cost: 0.02344974722800493\n",
            "Epoch: 1800, Cost: 0.021809427975418384\n",
            "Epoch: 1900, Cost: 0.02038109294311581\n",
            "Epoch: 2000, Cost: 0.019126398559693628\n",
            "Epoch: 2100, Cost: 0.018015684073635723\n",
            "Epoch: 2200, Cost: 0.01702565303239336\n",
            "Epoch: 2300, Cost: 0.016137758253834254\n",
            "Epoch: 2400, Cost: 0.015337053718511157\n",
            "Epoch: 2500, Cost: 0.014611363423631602\n",
            "Epoch: 2600, Cost: 0.013950669780946171\n",
            "Epoch: 2700, Cost: 0.013346656871810957\n",
            "Epoch: 2800, Cost: 0.012792364743634009\n",
            "Epoch: 2900, Cost: 0.012281924529321236\n",
            "Epoch: 3000, Cost: 0.011810353206294952\n",
            "Epoch: 3100, Cost: 0.01137339291940413\n",
            "Epoch: 3200, Cost: 0.010967383990433663\n",
            "Epoch: 3300, Cost: 0.010589163663650857\n",
            "Epoch: 3400, Cost: 0.010235984710964522\n",
            "Epoch: 3500, Cost: 0.00990544950145694\n",
            "Epoch: 3600, Cost: 0.00959545621800392\n",
            "Epoch: 3700, Cost: 0.00930415469095949\n",
            "Epoch: 3800, Cost: 0.009029909902971433\n",
            "Epoch: 3900, Cost: 0.008771271656969499\n",
            "Epoch: 4000, Cost: 0.008526949226575558\n",
            "Epoch: 4100, Cost: 0.008295790062324266\n",
            "Epoch: 4200, Cost: 0.008076761815449581\n",
            "Epoch: 4300, Cost: 0.007868937093223885\n",
            "Epoch: 4400, Cost: 0.007671480474179465\n",
            "Epoch: 4500, Cost: 0.007483637401320725\n",
            "Epoch: 4600, Cost: 0.007304724645664826\n",
            "Epoch: 4700, Cost: 0.007134122085675035\n",
            "Epoch: 4800, Cost: 0.006971265596934416\n",
            "Epoch: 4900, Cost: 0.006815640880958053\n",
            "Epoch: 5000, Cost: 0.006666778091219971\n",
            "Epoch: 5100, Cost: 0.006524247139331632\n",
            "Epoch: 5200, Cost: 0.0063876535829514285\n",
            "Epoch: 5300, Cost: 0.006256635013198897\n",
            "Epoch: 5400, Cost: 0.006130857872101842\n",
            "Epoch: 5500, Cost: 0.00601001464157239\n",
            "Epoch: 5600, Cost: 0.005893821354602017\n",
            "Epoch: 5700, Cost: 0.005782015386198394\n",
            "Epoch: 5800, Cost: 0.005674353487833891\n",
            "Epoch: 5900, Cost: 0.005570610035395837\n",
            "Epoch: 6000, Cost: 0.005470575463345312\n",
            "Epoch: 6100, Cost: 0.005374054862915194\n",
            "Epoch: 6200, Cost: 0.005280866724416716\n",
            "Epoch: 6300, Cost: 0.005190841806671982\n",
            "Epoch: 6400, Cost: 0.005103822118998603\n",
            "Epoch: 6500, Cost: 0.0050196600027659025\n",
            "Epoch: 6600, Cost: 0.004938217301349062\n",
            "Epoch: 6700, Cost: 0.0048593646089061075\n",
            "Epoch: 6800, Cost: 0.004782980589214317\n",
            "Epoch: 6900, Cost: 0.0047089513567434415\n",
            "Epoch: 7000, Cost: 0.004637169914465734\n",
            "Epoch: 7100, Cost: 0.004567535641077313\n",
            "Epoch: 7200, Cost: 0.0044999538237346605\n",
            "Epoch: 7300, Cost: 0.004434335231172272\n",
            "Epoch: 7400, Cost: 0.004370595723059854\n",
            "Epoch: 7500, Cost: 0.0043086558926799\n",
            "Epoch: 7600, Cost: 0.004248440738815126\n",
            "Epoch: 7700, Cost: 0.00418987936477996\n",
            "Epoch: 7800, Cost: 0.004132904701865033\n",
            "Epoch: 7900, Cost: 0.00407745325451037\n",
            "Epoch: 8000, Cost: 0.004023464865798948\n",
            "Epoch: 8100, Cost: 0.00397088250108741\n",
            "Epoch: 8200, Cost: 0.003919652048206885\n",
            "Epoch: 8300, Cost: 0.0038697221328830023\n",
            "Epoch: 8400, Cost: 0.0038210439479811507\n",
            "Epoch: 8500, Cost: 0.0037735710950812474\n",
            "Epoch: 8600, Cost: 0.003727259438005376\n",
            "Epoch: 8700, Cost: 0.0036820669667055224\n",
            "Epoch: 8800, Cost: 0.003637953670889092\n",
            "Epoch: 8900, Cost: 0.0035948814225256584\n",
            "Epoch: 9000, Cost: 0.003552813866549256\n",
            "Epoch: 9100, Cost: 0.003511716319096553\n",
            "Epoch: 9200, Cost: 0.003471555672607162\n",
            "Epoch: 9300, Cost: 0.003432300307441283\n",
            "Epoch: 9400, Cost: 0.003393920009258164\n",
            "Epoch: 9500, Cost: 0.003356385891856658\n",
            "Epoch: 9600, Cost: 0.0033196703251242654\n",
            "Epoch: 9700, Cost: 0.0032837468674451788\n",
            "Epoch: 9800, Cost: 0.0032485902026403533\n",
            "Epoch: 9900, Cost: 0.003214176080708428\n",
            "Epoch: 10000, Cost: 0.0031804812623268345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIV_GsoG2eDs"
      },
      "source": [
        "#### 테스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dpr0nZhc2Szr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4aefd6d-548f-4129-d59a-872c5d7bf694"
      },
      "source": [
        "print(XOR.predict(X))"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00141756]\n",
            " [0.99789224]\n",
            " [0.99858899]\n",
            " [0.0014194 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1IuDL8R7wrx"
      },
      "source": [
        "## 다중 클래스 분류 : MNIST Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CiJ5Gmq9Wpa"
      },
      "source": [
        "### 배치 처리\n",
        "- 학습 데이터 전체를 한번에 진행하지 않고  \n",
        "  일부 데이터(샘플)을 확률적으로 구해서 조금씩 나누어 진행\n",
        "\n",
        "- 확률적 경사 하강법(Stochastic Gradient Descent) 또는  \n",
        "  미니 배치 학습법(mini-batch learning)이라고도 부름"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUDNWwj49byH"
      },
      "source": [
        "#### 신경망 구현 : MNIST "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjBRQYlP74GM"
      },
      "source": [
        "#### 필요한 모듈 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0lJbkuW71lm"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDvtEiD77_gu"
      },
      "source": [
        "#### 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WL7zXMl_uo9"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_rNg5Jn8FRA"
      },
      "source": [
        "#### 데이터 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4wpsQGA8BOO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "280a9748-47ef-4222-b594-06fd15241f47"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU7nvkHO8IFR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "d001ff2c-8e53-4271-bb09-5c18be16144d"
      },
      "source": [
        "img = x_train[0]\n",
        "print(img.shape)\n",
        "\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbBA1Kl18KGT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a8bf0d-7ed7-4976-d870-ab8f1e49bd5f"
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTFu8i-z8U_C"
      },
      "source": [
        "#### 데이터 전처리 (Data Preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q76pjKDVftHJ"
      },
      "source": [
        "def flatten_for_mnist(x):\n",
        "    temp = np.zeros((x.shape[0], x[0].size))\n",
        "\n",
        "    for idx, data in enumerate(x):\n",
        "        temp[idx, :] = data.flatten()\n",
        "\n",
        "    return temp"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvMWrDOR8Mns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7d595df-75a8-43dd-c6f8-cfe32c4b3efe"
      },
      "source": [
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "x_train = flatten_for_mnist(x_train)\n",
        "x_test = flatten_for_mnist(x_test)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "y_train_ohe = tf.one_hot(y_train, depth=10).numpy()\n",
        "y_test_ohe = tf.one_hot(y_test, depth=10).numpy()\n",
        "\n",
        "print(y_train_ohe.shape)\n",
        "print(y_test_ohe.shape)"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n",
            "(60000, 10)\n",
            "(10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LjpWz0dotJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25610f63-c538-4533-bb1d-bec9b62ffbfa"
      },
      "source": [
        "print(x_train[0].max(), x_train[0].min())\n",
        "print(y_train_ohe[0])"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0 0.0\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GUaa92Y9RhY"
      },
      "source": [
        "#### 하이퍼 파라미터(Hyper Parameter)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk3FXXLi9Th5"
      },
      "source": [
        "epochs = 2\n",
        "lr = 0.1\n",
        "batch_size = 100\n",
        "train_size = x_train.shape[0]"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lMJ0h8p8iZl"
      },
      "source": [
        "#### 사용되는 함수들(Util Functions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSlqZ2Xx8hFn"
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def mean_squared_error(pred_y, true_y):\n",
        "    return 0.5 * (np.sum((true_y - pred_y)**2))\n",
        "\n",
        "def cross_entropy_error(pred_y, true_y):\n",
        "    if true_y.ndim == 1:\n",
        "        true_y = true_y.reshape(1, -1)\n",
        "        pred_y = pred_y.reshape(1, -1)\n",
        "\n",
        "    delta = 1e-7\n",
        "    return -np.sum(true_y * np.log(pred_y + delta)) / batch_size\n",
        "\n",
        "def cross_entropy_error_for_bin(pred_y, true_y):\n",
        "    return 0.5 * np.sum((-true_y * np.log(pred_y) - (1 - true_y) * np.log(1 - pred_y)))\n",
        "\n",
        "def softmax(a):\n",
        "    exp_a = np.exp(a)\n",
        "    sum_exp_a = np.sum(exp_a)\n",
        "    y = exp_a / sum_exp_a\n",
        "\n",
        "    return y\n",
        "\n",
        "def differential_1d(f, x):\n",
        "    \n",
        "    eps = 1e-5\n",
        "    diff_value = np.zeros_like(x)\n",
        "    \n",
        "    for i in range(x.shape[0]):\n",
        "        temp_val = x[i]\n",
        "\n",
        "        x[i] = temp_val + eps\n",
        "        f_h1 = f(x)\n",
        "\n",
        "        x[i] = temp_val - eps\n",
        "        f_h2 = f(x)\n",
        "\n",
        "        diff_value[i] = (f_h1 - f_h2) / (2 * eps)\n",
        "        x[i] = temp_val\n",
        "\n",
        "    return diff_value\n",
        "\n",
        "def differential_2d(f,X):\n",
        "    if X.ndim == 1:\n",
        "        return differential_1d(f, X)\n",
        "    else:\n",
        "        grad = np.zeros_like(X)\n",
        "\n",
        "        for idx, x in enumerate(X):\n",
        "            grad[idx] = differential_1d(f, x)\n",
        "\n",
        "        return grad"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSoV9fyj8_u7"
      },
      "source": [
        "#### 2층 신경망으로 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBObD5Fw89HI"
      },
      "source": [
        "class MyModel():\n",
        "    def __init__(self):\n",
        "\n",
        "        def weight_init(input_nodes, hidden_nodes, output_units):\n",
        "            np.random.seed(777)\n",
        "\n",
        "            params = {}\n",
        "            params['w_1'] = 0.01 * np.random.randn(input_nodes, hidden_nodes)\n",
        "            params['b_1'] = np.zeros(hidden_nodes)\n",
        "            params['w_2'] = 0.01 * np.random.randn(hidden_nodes, output_units)\n",
        "            params['b_2'] = np.zeros(output_units)\n",
        "            return params\n",
        "\n",
        "        self.params = weight_init(784, 64, 10)\n",
        "\n",
        "    def predict(self, x):\n",
        "        W_1, W_2 = self.params['w_1'], self.params['w_2']\n",
        "        B_1, B_2 = self.params['b_1'], self.params['b_2']\n",
        "\n",
        "        A1 = np.dot(x, W_1) + B_1\n",
        "        Z1 = sigmoid(A1)\n",
        "        A2 = np.dot(Z1, W_2) + B_2\n",
        "        pred_y = softmax(A2)\n",
        "\n",
        "        return pred_y\n",
        "\n",
        "    def loss(self, x, true_y):\n",
        "        pred_y = self.predict(x)\n",
        "        return cross_entropy_error_for_bin(pred_y, true_y)\n",
        "\n",
        "    def accuracy(self, x, true_y):\n",
        "        pred_y = self.predict(x)\n",
        "        y_argmax = np.argmax(pred_y, axis=1)\n",
        "        t_argmax = np.argmax(true_y, axis=1)\n",
        "\n",
        "        accuracy = np.sum(y_argmax == t_argmax) / float(x.shape[0])\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def get_gradient(self, x, t):\n",
        "\n",
        "        def loss_grad(grad):\n",
        "            return self.loss(x ,t)\n",
        "\n",
        "        grads = {}\n",
        "        grads['w_1'] = differential_2d(loss_grad, self.params['w_1'])\n",
        "        grads['b_1'] = differential_2d(loss_grad, self.params['b_1'])\n",
        "        grads['w_2'] = differential_2d(loss_grad, self.params['w_2'])\n",
        "        grads['b_2'] = differential_2d(loss_grad, self.params['b_2'])\n",
        "\n",
        "        return grads"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maKNIlK-xJ5k"
      },
      "source": [
        "#### 모델 생성 및 학습\n",
        "- 시간 많이 소요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSEARgNIop8t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "9c4cfddf282b446c8b74bde0ff2bb7ee",
            "2b1af583f36b496d96991172b0a10acc",
            "f268ca95424a4b2c8986383067c4dd1d",
            "ec825168fe9f4e88bdea6a009b7ef34e",
            "c624ef73b42e41c5afecdefb98b9e65a",
            "450e3d6e18a9439b929ec1c915235731",
            "c42d641ffb31453aac65a81f226049ee",
            "a0dce9b1bdaf46d2a81b1343829d11f8",
            "5587104c9365401eab15392f1e2ae030",
            "b33b79c07cde4b08a7460ef80bc969ec",
            "f25ba3d7266c42939a00b37f1ebcb475"
          ]
        },
        "outputId": "32d70d63-1526-40ad-b393-f0d3a728a180"
      },
      "source": [
        "model = MyModel()\n",
        "\n",
        "train_loss_list = list()\n",
        "train_acc_list = list()\n",
        "test_acc_list = list()\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "start_time = time.time()\n",
        "for i in tqdm(range(epochs)):\n",
        "\n",
        "    batch_idx = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_idx]\n",
        "    y_batch = y_train_ohe[batch_idx]\n",
        "\n",
        "    grads = model.get_gradient(x_batch, y_batch)\n",
        "\n",
        "    for key in grads.keys():\n",
        "        model.params[key] -= lr * grads[key]\n",
        "\n",
        "    loss = model.loss(x_batch, y_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    train_accuracy = model.accuracy(x_train, y_train_ohe)\n",
        "    test_accuracy = model.accuracy(x_test, y_test_ohe)\n",
        "    train_acc_list.append(train_accuracy)\n",
        "    test_acc_list.append(test_accuracy)\n",
        "\n",
        "    print(\"Epoch: {}, Cost: {}, Train Accuracy: {}, Test Accuracy: {}\".format(i+1, loss, train_accuracy, test_accuracy))\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"총 학습 소요시간: {:.3f}s\".format(end_time - start_time))"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c4cfddf282b446c8b74bde0ff2bb7ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Cost: 465.9060502356192, Train Accuracy: 0.10441666666666667, Test Accuracy: 0.1028\n",
            "Epoch: 2, Cost: 361.8440950235913, Train Accuracy: 0.09751666666666667, Test Accuracy: 0.0974\n",
            "총 학습 소요시간: 205.160s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7nL8f20x4zl"
      },
      "source": [
        "### 모델의 결과\n",
        "- 모델은 학습이 잘 될 수도, 잘 안될 수도 있음\n",
        "\n",
        "- 만약, 학습이 잘 되지 않았다면,  \n",
        "  학습이 잘 되기 위해서 어떠한 조치를 취해야 하는가?\n",
        "  - 다양한 학습관련 기술이 존재"
      ]
    }
  ]
}